---
title: "ê°•í™”í•™ìŠµ ê¸°ì´ˆ2: ì •ì±… ì´í„°ë ˆì´ì…˜, ê°€ì¹˜ ì´í„°ë ˆì´ì…˜"
date: 2021-01-24 13:00:23
categories:
  - Tech
  - ê°•í™”í•™ìŠµ
tags: [ReinforcementLearning, DeepLearning, PolicyIteration, ValueIteration]
mathjax: true
---

> ë³¸ í¬ìŠ¤íŠ¸ëŠ” "íŒŒì´ì¬ê³¼ ì¼€ë¼ìŠ¤ë¡œ ë°°ìš°ëŠ” ê°•í™”í•™ìŠµ" ë„ì„œì˜ ë‹¤ì„¯ë²ˆì§¸ ë¦¬ë·° í¬ìŠ¤íŠ¸ìž…ë‹ˆë‹¤.
> http://www.yes24.com/Product/Goods/44136413

## 3ìž¥ ê°•í™”í•™ìŠµ ê¸°ì´ˆ 2: ê·¸ë¦¬ë“œì›”ë“œì™€ ë‹¤ì´ë‚´ë¯¹ í”„ë¡œê·¸ëž˜ë°

### ì •ì±… ì´í„°ë ˆì´ì…˜ (Policy Iteration)

ê²°êµ­ MDPë¡œ ì •ì˜ë˜ëŠ” ë¬¸ì œì—ì„œ ì•Œê³  ì‹¶ì€ ê²ƒì€ **ê°€ìž¥ ë†’ì€ ë³´ìƒì„ ì–»ê²Œ í•˜ëŠ” ì •ì±…** ì„ ì°¾ëŠ” ê²ƒì´ë‹¤. í•˜ì§€ë§Œ ì²˜ìŒì—ëŠ” ì´ ì •ì±…ì„ ì•Œ ìˆ˜ê°€ ì—†ë‹¤. ë³´í†µ ì²˜ìŒì—ëŠ” ë¬´ìž‘ìœ„ë¡œ í–‰ë™ì„ ì •í•˜ëŠ” ì •ì±…ìœ¼ë¡œë¶€í„° ì‹œìž‘í•˜ì—¬ ê³„ì† ë°œì „ì‹œì¼œ ë‚˜ê°„ë‹¤.

í•˜ì§€ë§Œ ìš°ë¦¬ê°€ ì–»ê³ ìž í•˜ëŠ” ìµœì  ì •ì±…ì€ ë¬´ìž‘ìœ„ì •ì±…(random policy)ì´ ì•„ë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ í˜„ìž¬ì˜ ì •ì±…ì„ **í‰ê°€** í•˜ê³  ë” ë‚˜ì€ ì •ì±…ìœ¼ë¡œ **ë°œì „** í•´ì•¼í•œë‹¤. ì–´ë–¤ ì •ì±…ì´ ìžˆì„ ë•Œ `ì •ì±…í‰ê°€(Policy Evaluation)`ë¥¼ í†µí•´ ì–¼ë§ˆë‚˜ ì¢‹ì€ì§€ í‰ê°€í•˜ê³ , ê·¸ í‰ê°€ë¥¼ ê¸°ì¤€ìœ¼ë¡œ `ì •ì±…ë°œì „(Policy Improvement)`ì„ í†µí•´ ì¢€ ë” ë‚˜ì€ ì •ì±…ìœ¼ë¡œ ë°œì „ ì‹œí‚¨ë‹¤. ì´ëŸ¬í•œ ê³¼ì •ì„ ë¬´í•œížˆ ë°˜ë³µí•˜ë©´ **ì •ì±…ì€ ìµœì  ì •ì±…ìœ¼ë¡œ ìˆ˜ë ´** í•œë‹¤.

**ì•ˆì„ ìƒì˜ ì•ˆì†Œë¦¬ðŸ¤¬ ì˜ˆìƒ** : ë¬´ìž‘ìœ„ ì •ì±…ì—ì„œ ì¶œë°œí•´ì„œ ë„ëŒ€ì²´ ì–´ë–»ê²Œ ìµœì  ì •ì±…ì— ìˆ˜ë ´í•˜ì§€ìš”? ìžì„¸ížˆ ì„¤ëª…í•´ë³´ì„¸ìš”!
--> https://www.youtube.com/watch?v=rrTxOkbHj-M&t=19s íŒ¡ìš”ëž© ì˜ìƒì˜ 18ë¶„ ì¦ˆìŒë¶€í„° ì°¸ê³ 
(ì •ë¦¬í•´ë³´ìžë©´, ì£¼ìœ„ì˜ ìƒíƒœì˜ ê°’ë“¤ì€ ì“°ë ˆê¸° ê°’ì´ì§€ë§Œ **ë³´ìƒ** ê³¼ ê°™ì€ ìž‘ì€ ì •ë³´ë“¤ì„ í† ëŒ€ë¡œ ì ì§„ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ ë˜ì–´ ìµœì ì— ë‹¤ê°€ê°€ê²Œ ë˜ëŠ” ê²ƒì´ë‹¤. ì •ìˆ˜ê¸°ì—ì„œ ë¬¼ì´ ì •ìˆ˜ë˜ëŠ” ê²ƒì²˜ëŸ¼ ë§ì´ë‹¤! ì‚¬ì‹¤ ì•„ì§ë„ ì—…ë°ì´íŠ¸ê°€ ë˜ëŠ”ê²ƒì´ ì‹ ê¸°í•˜ë‹¤)

{% img /image/post_image/20210124/policy.png 'width:400px; height:250px' '' 'ì •ì±…í‰ê°€ì™€ ì •ì±…ë°œì „' %}

### ì •ì±… í‰ê°€ (Policy Evaluation)

ê°€ì¹˜í•¨ìˆ˜ëŠ” ì •ì±…ì´ ì–¼ë§ˆë‚˜ ì¢‹ì€ì§€ íŒë‹¨í•˜ëŠ” ê·¼ê±°ê°€ ëœë‹¤. ê°€ì¹˜í•¨ìˆ˜ëŠ” **í˜„ìž¬ ì •ì±… $\pi$ë¥¼ ë”°ë¼ê°”ì„ ë•Œ ë°›ì„ ë³´ìƒì— ëŒ€í•œ ê¸°ëŒ“ê°’**â­ï¸ ì´ë‹¤. ì—ì´ì „íŠ¸ì˜ ëª©í‘œëŠ” ì–´ë–»ê²Œ í•˜ë©´ ë³´ìƒì„ ë§Žì´ ë°›ì„ ìˆ˜ ìžˆì„ì§€ë¥¼ ì•Œì•„ë‚´ëŠ” ê²ƒì´ë¯€ë¡œ í˜„ìž¬ ì •ì±…ì— ë”°ë¼ ë°›ì„ ë³´ìƒì— ëŒ€í•œ ì •ë³´ê°€ ì •ì±…ì˜ ê°€ì¹˜ê°€ ë˜ëŠ” ê²ƒì´ë‹¤.

$$ v*\pi (s) = E[R*{t+1}+\gamma G\_{t+1}|S_t=s]$$

ì •ì±… ì´í„°ë ˆì´ì…˜ì—ì„œëŠ” ìœ„ì˜ ë²¨ë§Œ ê¸°ëŒ€ ë°©ì •ì‹ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì œë¥¼ í’€ ê²ƒì´ë‹¤. í•µì‹¬ì€ **ì£¼ë³€ ìƒíƒœì˜ ê°€ì¹˜í•¨ìˆ˜ì™€ í•œ íƒ€ìž„ìŠ¤í…ì˜ ë³´ìƒë§Œ ê³ ë ¤í•´ì„œ í˜„ìž¬ ìƒíƒœì˜ ë‹¤ìŒ ê°€ì¹˜í•¨ìˆ˜ë¥¼ ê³„ì‚°** í•˜ê² ë‹¤ëŠ” ê²ƒì´ë‹¤. ì´ ê³¼ì •ì€ í•œ íƒ€ìž„ìŠ¤í…ì˜ ë³´ìƒë§Œ ê³ ë ¤í•˜ê³  ì£¼ë³€ ìƒíƒœì˜ ê°€ì¹˜í•¨ìˆ˜ë“¤ì´ ì°¸ ê°€ì¹˜í•¨ìˆ˜ë“¤ì´ ì•„ë‹ˆê¸° ë•Œë¬¸ì— ì´ë ‡ê²Œ ê³„ì‚°í•´ë„ ì´ ê°’ì€ ì‹¤ì œê°’ì´ ì•„ë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ëŸ¬í•œ ê³„ì‚°ì„ **ì—¬ëŸ¬ë²ˆ ë°˜ë³µ** í•œë‹¤ë©´ **ì°¸ ê°’ìœ¼ë¡œ ìˆ˜ë ´** í•œë‹¤ëŠ” ê²ƒì´ ë©”ì¸ ì•„ì´ë””ì–´ì´ë‹¤.

ì´ì „ í¬ìŠ¤íŠ¸ì—ì„œ ìœ„ì˜ ì‹ì„ ê·¸ë¦¬ë“œì›”ë“œ ì˜ˆì œì—ì„œ ê³„ì‚° ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜í•´ ë³´ì•˜ì—ˆë‹¤. ì•„ëž˜ëŠ” kë²ˆì§¸ ê°€ì¹˜í•¨ìˆ˜ë¥¼ í†µí•´ k+1ë²ˆì§¸ ê°€ì¹˜í•¨ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” ì‹ì´ë‹¤. ìš°ë¦¬ëŠ” ì•„ëž˜ì˜ ì‹ì„ ë°˜ë³µì ìœ¼ë¡œ ê³„ì‚°í•´ ë‚˜ê°ˆ ê²ƒì´ë‹¤.

$$v_{k+1}(s) = \sum_{a \in A} \pi (a|s)(r_{(s,a)}+ \gamma v_k(s'))$$

#### í•œ ë²ˆì˜ ì •ì±… í‰ê°€ ê³¼ì •ì„ ìˆœì„œëŒ€ë¡œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

1. $k$ë²ˆì§¸ ê°€ì¹˜í•¨ìˆ˜ matrixì—ì„œ í˜„ìž¬ ìƒíƒœ $s$ì—ì„œ ê°ˆ ìˆ˜ ìžˆëŠ” ë‹¤ìŒìƒíƒœ $s'$ì— ì €ìž¥ë¼ ìžˆëŠ” ê°€ì¹˜í•¨ìˆ˜ $v_k(s')$ì„ ë¶ˆëŸ¬ì˜¨ë‹¤.
   (ë³´ë¼ìƒ‰ ë¶€ë¶„ì¤‘ì˜ í•˜ë‚˜)
2. $v_k(s')$ì— í• ì¸ìœ¨ $\gamma$ë¥¼ ê³±í•˜ê³  ê·¸ ìƒíƒœë¡œ ê°€ëŠ” í–‰ë™ì— ëŒ€í•œ ë³´ìƒ $R_s^a$ì„ ë”í•œë‹¤.
   $$r_{(s,a)} + \gamma v_k(s')$$
3. 2ë²ˆì—ì„œ êµ¬í•œ ê°’ì— ê·¸ í–‰ë™ì„ í•  í™•ë¥ , ì¦‰ ì •ì±… ê°’ì„ ê³±í•œë‹¤.
   $$\pi (a|s)(r_{(s,a)} + \gamma v_k(s'))$$
4. 3ë²ˆì„ ëª¨ë“  ì„ íƒ ê°€ëŠ¥í•œ í–‰ë™ì— ëŒ€í•´ ë°˜ë³µí•˜ê³  ê·¸ ê°’ë“¤ì„ ë”í•œë‹¤.
   $$v_{k+1}(s) = \sum_{a \in A} \pi (a|s)(r_{(s,a)}+ \gamma v_k(s'))$$
5. 4ë²ˆ ê³¼ì •ì„ í†µí•´ ë”í•œ ê°’ì„ $k+1$ë²ˆì§¸ ê°€ì¹˜í•¨ìˆ˜ matrixì˜ ìƒíƒœ $s$ìžë¦¬ì— ì €ìž¥í•œë‹¤.
6. 1-5 ê³¼ì •ì„ ëª¨ë“  $s \in S$ì— ëŒ€í•´ ë°˜ë³µí•œë‹¤.

{% img /image/post_image/20210124/policyIteration.png 'width:400px; height:200px' '' 'policy evaluation' %}

ì´ê²ƒì€ í•œë²ˆì˜ ì •ì±…í‰ê°€ ê³¼ì •ì´ë‹¤. í•˜ì§€ë§Œ í•œë²ˆì˜ ì •ì±…í‰ê°€ë¡œì„œëŠ” ì œëŒ€ë¡œ í‰ê°€ë¥¼ í•  ìˆ˜ ì—†ì–´, ì´ê³¼ì •ì„ ì—¬ëŸ¬ë²ˆ ë°˜ë³µí•˜ëŠ”ë°, $v_1$ìœ¼ë¡œ ì‹œìž‘í•´ì„œ ë¬´í•œížˆ ë°˜ë³µí•˜ë©´ ì°¸ $v_\pi$ê°€ ë  ìˆ˜ ìžˆë‹¤.

### ì •ì±… ë°œì „ (Policy Improvement)

ì• ì´ˆì— ì •ì±…ì„ ë°œì „ ì‹œí‚¤ì§€ ì•ŠëŠ”ë‹¤ë©´ ì •ì±…ì— ëŒ€í•œ í‰ê°€ëŠ” ì˜ë¯¸ê°€ ì—†ë‹¤. ê·¸ë ‡ë‹¤ë©´ ì •ì±… í‰ê°€ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì–´ë–»ê²Œ ì •ì±…ì„ ë°œì „ì‹œí‚¬ ìˆ˜ ìžˆì„ê¹Œ? ì‚¬ì‹¤ ì •ì±… ë°œì „ì˜ ë°©ë²•ì´ ì •í•´ì ¸ìžˆëŠ” ê²ƒì€ ì•„ë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ ì±…ì—ì„œëŠ” ê°€ìž¥ ë„ë¦¬ ì•Œë ¤ì§„ `íƒìš• ì •ì±… ë°œì „(Greedy Policy Improvement)`ì„ ì‚¬ìš©í•œë‹¤.

ì—ì´ì „íŠ¸ê°€ í•´ì•¼í•  ì¼ì€ ë‹¨ìˆœí•˜ë‹¤. ìƒíƒœ sì—ì„œ ì„ íƒ ê°€ëŠ¥í•œ í–‰ë™ì˜ $q_\pi (s,a)$ ì¦‰ íí•¨ìˆ˜ ê°’ì„ ë¹„êµí•˜ê³  ê·¸ì¤‘ì—ì„œ ê°€ìž¥ í° ê°’ì„ ê°€ì§€ëŠ” í–‰ë™ì„ ì„ íƒí•˜ë©´ ëœë‹¤. ì´ê²ƒì„ **íƒìš• ì •ì±… ë°œì „** ì´ë¼ê³  í•˜ëŠ”ë°, ëˆˆì•žì— ë³´ì´ëŠ” ê²ƒ ì¤‘ì—ì„œ **ë‹¹ìž¥ì— ê°€ìž¥ í° ì´ìµì„ ì¶”êµ¬í•˜ëŠ” ê²ƒê³¼ ê°™ì€ ëª¨ìŠµ** ì´ê¸° ë•Œë¬¸ì— ì´ëŸ¬í•œ ì´ë¦„ì´ ë¶™ì—ˆë‹¤.

íƒìš• ì •ì±… ë°œì „ì„ í†µí•´ ì—…ë°ì´íŠ¸ëœ ì •ì±…ì€ ì•„ëž˜ì™€ ê°™ë‹¤. max í•¨ìˆ˜ì™€ëŠ” ë‹¤ë¥´ê²Œ ë°˜í™˜ë˜ëŠ” ê²ƒì´ **í–‰ë™** ì´ë‹¤.
$$\pi'(s) = argmax_{a \in A} \\\; q_\pi (s,a)$$

{% img /image/post_image/20210124/greedy.png 'width:400px; height:200px' '' 'Greedy Policy Improvement' %}

íƒìš• ì •ì±… ë°œì „ì„ í†µí•´ ì •ì±…ì„ ì—…ë°ì´íŠ¸í•˜ë©´ ì´ì „ ê°€ì¹˜í•¨ìˆ˜ì— ë¹„í•´ ì—…ë°ì´íŠ¸ëœ ì •ì±…ìœ¼ë¡œ ì›€ì§ì˜€ì„ ë•Œ ë°›ì„ ê°€ì¹˜í•¨ìˆ˜ê°€ **ë¬´ì¡°ê±´ í¬ê±°ë‚˜ ê°™ë‹¤.** ë‹¤ì´íƒ€ë¯¹ í”„ë¡œê·¸ëž˜ë°ì—ì„œëŠ” ì´ì²˜ëŸ¼ íƒìš• ì •ì±… ë°œì „ì„ ì‚¬ìš©í•˜ì—¬ ê°€ìž¥ í°ê°’ì˜ ê°€ì¹˜í•¨ìˆ˜ë¥¼ ê°€ì§€ëŠ” ìµœì  ì •ì±…ì— ìˆ˜ë ´í•  ìˆ˜ ìžˆë‹¤.

### ì •ì±… ì´í„°ë ˆì´ì…˜ ì½”ë“œ

ìš°ì„  ì—ì´ì „íŠ¸ê°€ í•´ì•¼í•  ì—­í• ì„ ê³ ë ¤í•´ì„œ ì „ì²´ ì½”ë“œì˜ íë¦„ì„ ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

```python
class PolicyIteration:
  def __init__(self, env):
    # í™˜ê²½ì— ëŒ€í•œ ê°ì²´
    self.env = env

  # ì •ì±… í‰ê°€
  def policy_evaluation(self):
    pass

  # ì •ì±… ë°œì „
  def policy_improvement(self):
    pass

  # íŠ¹ì • ìƒíƒœì—ì„œ ì •ì±…ì— ë”°ë¥¸ í–‰ë™
  def get_action(self, state):
    return action

if __name__ == "__main__":
  env = Env()
  policy_iteration = PolicyIteration(env)
  grid_world = GraphicDisplay(policy_iteration)
  grid_world.mainloop()

```

ì—ì´ì „íŠ¸ê°€ ì•Œê³  ìžˆëŠ” í™˜ê²½(env)ì˜ ì •ë³´ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

- `env.width, env.height`: ê·¸ë¦¬ë“œì›”ë“œì˜ ë„ˆë¹„ì™€ ë†’ì´
- `env.state_after_aciton(state, action)`: íŠ¹ì • ìƒíƒœì—ì„œ íŠ¹ì • í–‰ë™ì„ í–ˆì„ ë•Œ ì—ì´ì „íŠ¸ê°€ ê°€ëŠ” ë‹¤ìŒ ìƒíƒœ
- `env.get_all_states()`: ì¡´ìž¬í•˜ëŠ” ëª¨ë“  ìƒíƒœ
- `env.get_reward(state, action)`: íŠ¹ì • ìƒíƒœì˜ ë³´ìƒ
- `env.possible_actions`: ìƒ,í•˜,ì¢Œ,ìš°

ì •ì±… ì´í„°ë ˆì´ì…˜ì€ **ì •ì±… í‰ê°€** ì™€ **ì •ì±… ë°œì „** ìœ¼ë¡œ ì´ë¤„ì ¸ ìžˆë‹¤. ë”°ë¼ì„œ ê°ê°ì˜ í•¨ìˆ˜ë¥¼ ì •ì˜ í•œë‹¤.

#### policy_evaluation

ì •ì±… í‰ê°€ë¥¼ í†µí•´ ì—ì´ì „íŠ¸ëŠ” ëª¨ë“  ìƒíƒœì˜ ê°€ì¹˜í•¨ìˆ˜ë¥¼ ì—…ë°ì´íŠ¸ í•œë‹¤. ëª¨ë“  ìƒíƒœì— ëŒ€í•´ **ë²¨ë°˜ ê¸°ëŒ€ ë°©ì •ì‹** ì˜ ê³„ì‚°ì´ ëë‚˜ë©´ í˜„ìž¬ì˜ value_tableì— next_value_tableì„ ë®ì–´ì“°ëŠ” ì‹ìœ¼ë¡œ policy_evalutationì„ ì§„í–‰í•œë‹¤.

$$v_{k+1}(s) = \sum_{a \in A} \pi (a|s)(r_{(s,a)}+ \gamma v_k(s'))$$

ì •ì±… í‰ê°€ì— ì‚¬ìš©ë˜ëŠ” ë²¨ë§Œ ê¸°ëŒ€ ë°©ì •ì‹ì´ë‹¤. ì´ë•Œ, **ìƒíƒœ ë³€í™˜ í™•ë¥ ** ì„ 1ì´ë¼ê³  ì„¤ì • í–ˆê¸° ë•Œë¬¸ì—, ë‹¤ìŒìƒíƒœ $s'$ì€ ë§Œì•½ í–‰ë™ì´ ì™¼ìª½ì¼ ê²½ìš° ì™¼ìª½ì— ìžˆëŠ” ìƒíƒœê°€ ëœë‹¤.

```python
# ë²¨ë§Œ ê¸°ëŒ€ ë°©ì •ì‹ì„ í†µí•´ ë‹¤ìŒ ê°€ì¹˜í•¨ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” ì •ì±… í‰ê°€
def policy_evaluation(self):
    # ë‹¤ìŒ ê°€ì¹˜í•¨ìˆ˜ ì´ˆê¸°í™”
    next_value_table = [[0.00] * self.env.width
                        for _ in range(self.env.height)]

    # ëª¨ë“  ìƒíƒœì— ëŒ€í•´ì„œ ë²¨ë§Œ ê¸°ëŒ€ë°©ì •ì‹ì„ ê³„ì‚°
    for state in self.env.get_all_states():
        value = 0.0
        # ë§ˆì¹¨ ìƒíƒœì˜ ê°€ì¹˜ í•¨ìˆ˜ = 0
        if state == [2, 2]:
            next_value_table[state[0]][state[1]] = value
            continue

        # ë²¨ë§Œ ê¸°ëŒ€ ë°©ì •ì‹
        for action in self.env.possible_actions:
            next_state = self.env.state_after_action(state, action)
            reward = self.env.get_reward(state, action)
            next_value = self.get_value(next_state)
            value += (self.get_policy(state)[action] *
                      (reward + self.discount_factor * next_value))

        next_value_table[state[0]][state[1]] = value

    self.value_table = next_value_table
```

- $\sum_{a \in A}$: `for action in self.env.possible_actions:`
- $\pi (a|s)$: `self.get_policy(state)[action]`
- $r(s,a)$: `reward = self.env.get_reward(state, action)`
- $\gamma$: `self.discount_factor` (0.9)
- $s'$: `next_state = self.env.state_after_action(state, action)`
- $v_k(s')$: `self.get_value(next_state)`

- $v_{k+1}(s)$: `next_value_table[state[0]][state[1]]`

get_policy í•¨ìˆ˜ë¥¼ í†µí•´ ê° ìƒíƒœì—ì„œ ê° í–‰ë™ì— ëŒ€í•œ í™•ë¥ ê°’ì„ êµ¬í•œë‹¤. ê·¸ë¦¬ê³  ë‹¤ìŒ ìƒíƒœë¡œ ê°”ì„ ë•Œ ë°›ì„ ë³´ìƒê³¼ ë‹¤ìŒìƒíƒœì˜ ê°€ì¹˜í•¨ìˆ˜ë¥¼ í• ì¸ìœ¨ì„ ì ìš©í•˜ì—¬ ë”í•œë‹¤. ì •ì±…ì´ ê° í–‰ë™ì— ëŒ€í•œ í™•ë¥ ì„ ë‚˜íƒ€ë‚´ê¸° ë•Œë¬¸ì— ëª¨ë“  í–‰ë™ì— ëŒ€í•´ valueë¥¼ ê³„ì‚°í•˜ê³  ë”í•˜ë©´ ê¸°ëŒ“ê°’ì„ ê³„ì‚°í•œ ê²ƒì´ ëœë‹¤.

#### policy_improvement

ì •ì±… í‰ê°€ë¥¼ í†µí•´ ì •ì±…ì„ í‰ê°€í•˜ë©´ ê·¸ì— ë”°ë¥¸ ìƒˆë¡œìš´ ê°€ì¹˜í•¨ìˆ˜ë¥¼ ì–»ëŠ”ë‹¤. ì—ì´ì „íŠ¸ëŠ” ì´ì œ ìƒˆë¡œìš´ ê°€ì¹˜í•¨ìˆ˜ë¥¼ í†µí•´ ì •ì±…ì„ ì—…ë°ì´íŠ¸ í•´ì•¼í•œë‹¤. ì •ì±…ë°œì „ì—ëŠ” íƒìš• ì •ì±… ë°œì „ì„ ì‚¬ìš©í•œë‹¤.

```python
# í˜„ìž¬ ê°€ì¹˜ í•¨ìˆ˜ì— ëŒ€í•´ì„œ íƒìš• ì •ì±… ë°œì „
def policy_improvement(self):
    next_policy = self.policy_table
    for state in self.env.get_all_states():
        if state == [2, 2]:
            continue

        value_list = []
        # ë°˜í™˜í•  ì •ì±… ì´ˆê¸°í™”
        result = [0.0, 0.0, 0.0, 0.0]

        # ëª¨ë“  í–‰ë™ì— ëŒ€í•´ì„œ [ë³´ìƒ + (í• ì¸ìœ¨ * ë‹¤ìŒ ìƒíƒœ ê°€ì¹˜í•¨ìˆ˜)] ê³„ì‚°
        for index, action in enumerate(self.env.possible_actions):
            next_state = self.env.state_after_action(state, action)
            reward = self.env.get_reward(state, action)
            next_value = self.get_value(next_state)
            value = reward + self.discount_factor * next_value
            value_list.append(value)

        # ë°›ì„ ë³´ìƒì´ ìµœëŒ€ì¸ í–‰ë™ë“¤ì— ëŒ€í•´ íƒìš• ì •ì±… ë°œì „
        max_idx_list = np.argwhere(value_list == np.amax(value_list))
        max_idx_list = max_idx_list.flatten().tolist()
        prob = 1 / len(max_idx_list)

        for idx in max_idx_list:
            result[idx] = prob

        next_policy[state[0]][state[1]] = result

    self.policy_table = next_policy
```

íƒìš• ì •ì±… ë°œì „ì€ ê°€ì¹˜ê°€ ê°€ìž¥ ë†’ì€ í•˜ë‚˜ì˜ í–‰ë™ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ë‹¤. í•˜ì§€ë§Œ ì´ ì˜ˆì œì—ì„œì™€ ê°™ì´ í˜„ìž¬ ìƒíƒœì—ì„œ ê°€ìž¥ ì¢‹ì€ í–‰ë™ì´ ì—¬ëŸ¬ê°œì¼ ìˆ˜ë„ ìžˆë‹¤. ê·¸ëŸ´ë•Œì—ëŠ” ê°€ìž¥ ì¢‹ì€ í–‰ë™ë“¤ì„ ë™ì¼í•œ í™•ë¥ ë¡œ ì„ íƒí•˜ëŠ” ì •ì±…ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•œë‹¤.

- í˜„ìž¬ ìƒíƒœì—ì„œ ê°€ëŠ¥í•œ í–‰ë™ì— ëŒ€í•´ $r_{(s,a)}+\gamma v_k(s')$ì„ ê³„ì‚°í•œë‹¤.
- ê³„ì‚°í•œ ê°’ì„ value_listì— ì €ìž¥í•œë‹¤.
- max í•¨ìˆ˜ë¥¼ í†µí•´ value_listì— ë‹´ê¸´ ê°’ ì¤‘ ê°€ìž¥ í° ê°’ì„ ì•Œì•„ë‚¸ë‹¤.
- argwhereë¥¼ í†µí•´ ê°€ìž¥ í° ê°’ì˜ indexë¥¼ ì•Œì•„ë‚´ê³ (ì—¬ëŸ¬ê°œë©´ ì—¬ëŸ¬ê°œ ë°˜í™˜) max_idx_listì— ì €ìž¥í•œë‹¤.
- max_idx_listì˜ ê¸¸ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í™•ë¥ ì„ ê³„ì‚°í•˜ì—¬ í™•ë¥ ê°’ì„ ê³„ì‚°í•˜ê³  resultì— ì €ìž¥í•œë‹¤.

ì´ë ‡ê²Œ ë˜ë©´ policy_tableì—ëŠ” ì—…ë°ì´íŠ¸ ëœ ì •ì±…(ê° ìƒíƒœì˜ í–‰ë™ì— ëŒ€í•œ í™•ë¥ )ì´ ì €ìž¥ëœë‹¤.

> ì •ì±… ì´í„°ë ˆì´ì…˜ì—ì„œ ì—ì´ì „íŠ¸ëŠ” ì •ì±… í‰ê°€ì™€ ì •ì±… ë°œì „ì„ ë°˜ë³µí•˜ì—¬ ìµœì  ì •ì±…ì„ ì°¾ì•„ë‚¸ë‹¤.

{% img /image/post_image/20210124/optimalpolicy.png 'width:300px; height:300px' '' 'ì •ì±… ì´í„°ë ˆì´ì…˜ìœ¼ë¡œ êµ¬í•œ ìµœì  ì •ì±…' %}

### ê°€ì¹˜ ì´í„°ë ˆì´ì…˜

ì •ì±… ì´í„°ë ˆì´ì…˜ê³¼ ê°€ì¹˜ ì´í„°ë ˆì´ì…˜ì˜ ì¤‘ìš”í•œ ì°¨ì´ì ì€ ì •ì±… ì´í„°ë ˆì´ì…˜ì—ì„œëŠ” **ì •ì±… í‰ê°€ì™€ ì •ì±… ë°œì „** ìœ¼ë¡œ ë‹¨ê³„ê°€ ë‚˜ëˆ„ì–´ì ¸ ìžˆë‹¤ë©´ ê°€ì¹˜ ì´í„°ë ˆì´ì…˜ì—ì„œëŠ” ê·¸ë ‡ì§€ ì•Šë‹¤ëŠ” ê²ƒì´ë‹¤.

ê°€ì¹˜ ì´í„°ë ˆì´ì…˜ì€ í˜„ìž¬ì˜ ê°€ì¹˜í•¨ìˆ˜ê°€ ìµœì  ì •ì±…ì— ëŒ€í•œ ê°€ì¹˜í•¨ìˆ˜ë¼ê³  ê°€ì •í•˜ê¸° ë•Œë¬¸ì— ì •ì±…ì„ ë°œì „í•˜ëŠ” í•¨ìˆ˜ê°€ ë”°ë¡œ í•„ìš”í•˜ì§€ ì•Šë‹¤. ë”°ë¼ì„œ ìµœì  í–‰ë™ì„ ë°˜í™˜í•˜ëŠ” get_action í•¨ìˆ˜ë¥¼ ì •ì±…ì„ ì¶œë ¥í•˜ëŠ” ë° ëŒ€ì‹  ì‚¬ìš©í•œë‹¤.

ì£¼ìš” ì½”ë“œì˜ ì „ì²´ì ì¸ íë¦„ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

```python
class ValueIteration:
  def __init__(self, env):
    # í™˜ê²½ ê°ì²´ ìƒì„±
    self.env = env

  # ë²¨ë§Œ ìµœì  ë°©ì •ì‹ì„ í†µí•´ ë‹¤ìŒ ê°€ì¹˜í•¨ìˆ˜ ê³„ì‚°
  def value_iteration(self):
    return

  # í˜„ìž¬ ê°€ì¹˜í•¨ìˆ˜ë¡œë¶€í„° í–‰ë™ì„ ë°˜í™˜
  def get_action(self, state):
    return

  def get_value(self, state):
    return

if __name__ == "__main__":
  env = Env()
  value_iteration = ValueIteration(env)
  grid_world = GraphicDisplay(value_iteration)
  gird_world.mainloop()
```

ì •ì±… ì´í„°ë ˆì´ì…˜ì—ì„œëŠ” policy_evaluation í•¨ìˆ˜ì—ì„œ ë²¨ë§Œ ê¸°ëŒ€ ë°©ì •ì‹ì„ í†µí•´ ë‹¤ìŒ ê°€ì¹˜í•¨ìˆ˜ë¥¼ ê³„ì‚°í–ˆë‹¤. ê°€ì¹˜ ì´í„°ë ˆì´ì…˜ì—ì„œëŠ” value_iteration í•¨ìˆ˜ë¥¼ í†µí•´ ë‹¤ìŒ ê°€ì¹˜í•¨ìˆ˜ë¥¼ ê³„ì‚°í•œë‹¤.

```python
# ë²¨ë§Œ ìµœì  ë°©ì •ì‹ì„ í†µí•´ ë‹¤ìŒ ê°€ì¹˜ í•¨ìˆ˜ ê³„ì‚°
def value_iteration(self):
    # ë‹¤ìŒ ê°€ì¹˜í•¨ìˆ˜ ì´ˆê¸°í™”
    next_value_table = [[0.0] * self.env.width
                        for _ in range(self.env.height)]

    # ëª¨ë“  ìƒíƒœì— ëŒ€í•´ì„œ ë²¨ë§Œ ìµœì ë°©ì •ì‹ì„ ê³„ì‚°
    for state in self.env.get_all_states():
        # ë§ˆì¹¨ ìƒíƒœì˜ ê°€ì¹˜ í•¨ìˆ˜ = 0
        if state == [2, 2]:
            next_value_table[state[0]][state[1]] = 0.0
            continue

        # ë²¨ë§Œ ìµœì  ë°©ì •ì‹
        value_list = []
        for action in self.env.possible_actions:
            next_state = self.env.state_after_action(state, action)
            reward = self.env.get_reward(state, action)
            next_value = self.get_value(next_state)
            value_list.append((reward + self.discount_factor * next_value))

        # ìµœëŒ“ê°’ì„ ë‹¤ìŒ ê°€ì¹˜ í•¨ìˆ˜ë¡œ ëŒ€ìž…
        next_value_table[state[0]][state[1]] = max(value_list)

    self.value_table = next_value_table
```

$$v_{k+1}(s) = max_{a \in A}(r_{s,a} + \gamma v_k(s'))$$

- ìœ„ì˜ ë²¨ë§Œ ìµœì  ë°©ì •ì‹ì„ ê³„ì‚°í•˜ì—¬ value_listì— ì €ìž¥í•œë‹¤.
- value_listì— ì €ìž¥ëœ ê°’ì¤‘ ìµœëŒ€ì˜ ê°’ì„ ìƒˆë¡œìš´ ê°€ì¹˜í•¨ìˆ˜ë¡œ ì €ìž¥í•œë‹¤.

```python
# í˜„ìž¬ ê°€ì¹˜ í•¨ìˆ˜ë¡œë¶€í„° í–‰ë™ì„ ë°˜í™˜
def get_action(self, state):
    if state == [2, 2]:
        return []

    # ëª¨ë“  í–‰ë™ì— ëŒ€í•´ íí•¨ìˆ˜ (ë³´ìƒ + (ê°ê°€ìœ¨ * ë‹¤ìŒ ìƒíƒœ ê°€ì¹˜í•¨ìˆ˜))ë¥¼ ê³„ì‚°
    value_list = []
    for action in self.env.possible_actions:
        next_state = self.env.state_after_action(state, action)
        reward = self.env.get_reward(state, action)
        next_value = self.get_value(next_state)
        value = (reward + self.discount_factor * next_value)
        value_list.append(value)

    # ìµœëŒ€ í í•¨ìˆ˜ë¥¼ ê°€ì§„ í–‰ë™(ë³µìˆ˜ì¼ ê²½ìš° ì—¬ëŸ¬ ê°œ)ì„ ë°˜í™˜
    max_idx_list = np.argwhere(value_list == np.amax(value_list))
    action_list = max_idx_list.flatten().tolist()
    return action_list
```

- ëª¨ë“  í–‰ë™ì— ëŒ€í•´ íí•¨ìˆ˜ë¥¼ êµ¬í•œë‹¤. $r_{s,a} + \gamma v_k(s')$
- ê·¸ ì¤‘ ê°€ìž¥ í° value ê°’ì„ ê°€ì§€ëŠ” í–‰ë™ì˜ ì¸ë±ìŠ¤ë¥¼ ê°€ì ¸ì˜¨ë‹¤(ì—¬ëŸ¬ê°œë¼ë©´ ì—¬ëŸ¬ê°œ ëª¨ë‘ ê°€ì ¸ì˜¨ë‹¤)
- í–‰ë™ë“¤ì„ ëª¨ë‘ action_listì— ì €ìž¥í•œë‹¤.

ì´ ì˜ˆì œì— ëŒ€í•´ì„œëŠ” Calculate 6ë²ˆ ì •ë„ì— ê°€ì¹˜í•¨ìˆ˜ê°€ ê±°ì˜ ìˆ˜ë ´í•˜ê²Œ ëœë‹¤. ì´ë•Œ ìˆ˜ë ´í•œ ê°€ì¹˜í•¨ìˆ˜ì˜ ê°’ì„ í† ëŒ€ë¡œ ì •ì±…ì„ ì¶œë ¥í•´ë³´ë©´ ì•„ëž˜ì˜ ê·¸ë¦¼ê³¼ ê°™ë‹¤.

{% img /image/post_image/20210124/valueiteration.png 'width:300px; height:300px' '' 'ê°€ì¹˜ ì´í„°ë ˆì´ì…˜ìœ¼ë¡œ êµ¬í•œ ìµœì  ì •ì±…' %}

### ë‹¤ì´ë‚´ë¯¹ í”„ë¡œê·¸ëž˜ë°ì˜ í•œê³„ì™€ ê°•í™”í•™ìŠµ

ë²¨ë§Œ ë°©ì •ì‹ì„ ì´ìš©í•œ ë‹¤ì´ë‚´ë¯¹ í”„ë¡œê·¸ëž˜ë°ìœ¼ë¡œì„œ ì •ì±… ì´í„°ë ˆì´ì…˜ê³¼ ê°€ì¹˜ ì´í„°ë ˆì´ì…˜ì„ ì‚´íŽ´ë´¤ë‹¤. í•˜ì§€ë§Œ ë‹¤ì´ë‚´ë¯¹ í”„ë¡œê·¸ëž˜ë°ì€ **ê³„ì‚°ì„ ë¹ ë¥´ê²Œ í•˜ëŠ” ê²ƒì´ì§€ í•™ìŠµì„ í•˜ëŠ” ê²ƒ** ì€ ì•„ë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ì´ëŸ¬í•œ ë‹¤ì´ë‚´ë¯¹ í”„ë¡œê·¸ëž˜ë°ì˜ í•œê³„ëŠ” ë¬´ì—‡ì¼ê¹Œ?

**1. ê³„ì‚°ë³µìž¡ë„** : ë¬¸ì œì˜ ê·œëª¨ê°€ ì»¤ì§€ë§Œ ê³„ì‚°ìœ¼ë¡œ í‘¸ëŠ”ë°ì— í•œê³„. `ê³„ì‚°ë³µìž¡ë„` = ìƒíƒœ í¬ê¸°ì˜ 3ì œê³±
**2. ì°¨ì›ì˜ ì €ì£¼** : ê·¸ë¦¬ë“œì›”ë“œì˜ ìƒíƒœì˜ ì°¨ì›ì€ 2ì°¨ì›. ìƒíƒœì˜ ì°¨ì›ì´ ëŠ˜ì–´ë‚˜ë©´ ìƒíƒœì˜ ìˆ˜ê°€ ì§€ìˆ˜ì ìœ¼ë¡œ ì¦ê°€
**3. í™˜ê²½ì— ëŒ€í•œ ì™„ë²½í•œ ì •ë³´ê°€ í•„ìš”** : ë³´ìƒê³¼ ìƒíƒœ ë³€í™˜ í™•ë¥ ì„ ì •í™•ížˆ ì•ˆë‹¤ëŠ” ê°€ì • í•„ìš”. ë³´í†µì€ ì´ ì •ë³´ë¥¼ í™•ì‹¤ížˆ ì•Œ ìˆ˜ ì—†ìŒ.

í˜„ì‹¤ì„¸ê³„ì˜ í™˜ê²½ì— ë†“ì¸ ë¬¸ì œë¥¼ í’€ì–´ë‚´ëŠ” ë°ì—ëŠ” ìœ„ì˜ ì„¸ ê°€ì§€ í•œê³„ê°€ ì¹˜ëª…ì ìœ¼ë¡œ ìž‘ìš©í•œë‹¤. ë•Œë¬¸ì— ì´ëŸ¬í•œ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ í™˜ê²½ì„ ëª¨ë¥´ì§€ë§Œ **í™˜ê²½ê³¼ì˜ ìƒí˜¸ìž‘ìš©ì„ í†µí•´ ê²½í—˜ì„ ë°”íƒ•ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ë°©ë²•** ì´ ë“±ìž¥í•œë‹¤. ë°”ë¡œ `ê°•í™”í•™ìŠµ`ì´ë‹¤.

### ëª¨ë¸ì—†ì´ í•™ìŠµí•˜ëŠ” ê°•í™”í•™ìŠµ

í™˜ê²½ì˜ ëª¨ë¸ì´ëž€ ë¬´ì—‡ì¼ê¹Œ? MDPì—ì„œ í™˜ê²½ì˜ ëª¨ë¸ì€ ìƒíƒœ ë³€í™˜ í™•ë¥ ê³¼ ë³´ìƒì´ë‹¤.

$$Model = P_{ss'}^a, \\\; r(s,a)$$

í˜„ìž¬ ì´ ì±…ì—ì„œ ë‹¤ë£¨ê³  ì‹¶ì€ ëª¨ë¸ì€ **ìˆ˜í•™ì  ëª¨ë¸** ë¡œì„œ ì‹œìŠ¤í…œì— ìž…ë ¥ì´ ë“¤ì–´ì™”ì„ ë•Œ ì‹œìŠ¤í…œì´ ì–´ë–¤ ì¶œë ¥ì„ ë‚´ëŠ”ì§€ì— ëŒ€í•œ ë°©ì •ì‹ì´ë‹¤. ì´ì²˜ëŸ¼ ìž…ë ¥ê³¼ ì¶œë ¥ì˜ ê´€ê³„ë¥¼ ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ëŠ” ê³¼ì •ì„ `ëª¨ë¸ë§(Modeling)`ì´ë¼ê³  í•œë‹¤.

ì‚¬ì‹¤ ìž…ë ¥ê³¼ ì¶œë ¥ ì‚¬ì´ì˜ ë°©ì •ì‹ì€ ì •í™•í•  ìˆ˜ê°€ ì—†ë‹¤. ë°©ì •ì‹ì—ì„œëŠ” Aë¼ëŠ” ìž…ë ¥ì´ ë“¤ì–´ì™€ì„œ Bë¼ëŠ” ì¶œë ¥ì´ ë‚˜ì˜¤ë”ë¼ë„ ì‹¤ì œ ì„¸ìƒì—ì„œëŠ” Bë¼ëŠ” ì¶œë ¥ì´ ì ˆëŒ€ë¡œ ë‚˜ì˜¤ì§€ ì•ŠëŠ”ë‹¤. ëª¨ë¸ì€ ì •í™•í•˜ë©´ ì •í™•í• ìˆ˜ë¡ ë³µìž¡í•˜ë©° ê³µê¸°ë‚˜ ë°”ëžŒê°™ì€ **ìžì—°í˜„ìƒì„ ì •í™•í•˜ê²Œ ëª¨ë¸ë§í•˜ëŠ” ê²ƒì€ ë¶ˆê°€ëŠ¥** ì— ê°€ê¹ë‹¤. ê²Œìž„ì—ì„œëŠ” ì‚¬ì‹¤ ì‚¬ëžŒì´ í™˜ê²½ì„ ë§Œë“¤ì—ˆê³ , ì‚¬ëžŒì´ ì •í•´ì¤€ëŒ€ë¡œë§Œ ê²Œìž„ì´ ìž‘ë™í•˜ë¯€ë¡œ **ëª¨ë¸ë§ ì˜¤ì°¨ëŠ” ì—†ë‹¤** ê³  ë³¼ ìˆ˜ ìžˆë‹¤. í•˜ì§€ë§Œ ê²Œìž„ì„ ë²—ì–´ë‚œë‹¤ë©´ ê¸€ìŽ„...ðŸ¤”

ëª¨ë¸ì„ ì •í™•ížˆ ì•Œê¸° ì–´ë ¤ìš´ ê²½ìš°, ì‹œìŠ¤í…œì˜ ìž…ë ¥ê³¼ ì¶œë ¥ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ì•Œê¸° ìœ„í•´ ë‘ê°€ì§€ ì ‘ê·¼ ë°©ë²•ìœ¼ë¡œ ì ‘ê·¼í•´ ë³¼ ìˆ˜ ìžˆë‹¤.

1. í•  ìˆ˜ ìžˆëŠ” ì„ ì—ì„œ ì •í™•í•œ ëª¨ë¸ë§ì„ í•œ ë‹¤ìŒ, ëª¨ë¸ë§ ì˜¤ì°¨ì— ëŒ€í•œ ë¶€ë¶„ì„ ì‹¤í—˜ì„ í†µí•´ ì¡°ì •í•œë‹¤.
2. ëª¨ë¸ ì—†ì´ í™˜ê²½ê³¼ì˜ ìƒí˜¸ìž‘ìš©ì„ í†µí•´ ìž…ë ¥ê³¼ ì¶œë ¥ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ í•™ìŠµí•œë‹¤.

1ë²ˆì€ í•™ìŠµì˜ ê°œë…ì—†ì´ ê³ ì „ì ìœ¼ë¡œ ë§Žì´ ì ìš©í•˜ëŠ” ë°©ë²•ì´ë©° ì‹œìŠ¤í…œì˜ ì•ˆì •ì„±ì„ ë³´ìž¥í•œë‹¤. í•˜ì§€ë§Œ ë¬¸ì œê°€ ë³µìž¡í•´ì§€ê³  ì–´ë ¤ì›Œì§ˆìˆ˜ë¡ í•œê³„ê°€ ìžˆë‹¤.

2ë²ˆì€ í•™ìŠµì˜ ê°œë…ì´ ë“¤ì–´ê°„ë‹¤. í•™ìŠµì˜ íŠ¹ì„±ìƒ ëª¨ë“  ìƒí™©ì—ì„œ ë™ì¼í•˜ê²Œ ìž‘ë™í•œë‹¤ê³  ë³´ìž¥í•  ìˆ˜ ì—†ì§€ë§Œ ë§Žì€ ë³µìž¡í•œ ë¬¸ì œì—ì„œ ëª¨ë¸ì´ í•„ìš”ì—†ëŠ” ê²ƒì€ ìž¥ì ì´ë‹¤. 2ë²ˆ ë°©ë²•ì´ ë°”ë¡œ ì´ ì±…ì˜ ì£¼ì œì¸ `ê°•í™”í•™ìŠµ` ì´ë‹¤.

### ì •ë¦¬

- `ì •ì±… ì´í„°ë ˆì´ì…˜`: ë²¨ë§Œ ê¸°ëŒ€ ë°©ì •ì‹ì„ ì´ìš©í•´ ì •ì±…ì„ í‰ê°€í•˜ê³ , íƒìš• ì •ì±… ë°œì „ì„ ì´ìš©í•´ ì •ì±… ë°œì „
- `ê°€ì¹˜ ì´í„°ë ˆì´ì…˜`: ìµœì  ì •ì±…ì„ ê°€ì •í•˜ê³  ë²¨ë§Œ ìµœì  ë°©ì •ì‹ ì´ìš©. ì •ì±…ì´ ì§ì ‘ì ìœ¼ë¡œ ì£¼ì–´ì§€ì§€ ì•Šì•„ íí•¨ìˆ˜ë¥¼ í†µí•´ í–‰ë™ ì„ íƒ
- `ë‹¤ì´ë‚´ë¯¹ í”„ë¡œê·¸ëž˜ë°ì˜ í•œê³„`: ê³„ì‚° ë³µìž¡ë„, ì°¨ì›ì˜ ì €ì£¼, í™˜ê²½ì— ëŒ€í•œ ì™„ë²½í•œ ì •ë³´ í•„ìš”

### 3ìž¥ í•œì¤„í‰

> ì§§ê³  ê°„ê²°í•˜ê²Œ ì“°ëŠ”ê²Œ ì˜¤ížˆë ¤ íž˜ë“¤ë‹¤..
