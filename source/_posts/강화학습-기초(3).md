---
title: "강화학습 기초2: 그리드월드와 다이내믹 프로그래밍"
date: 2021-01-23 12:58:43
categories:
  - Tech
  - 강화학습
tags: [ReinforcementLearning, DynamicProgramming, GridWorld]
mathjax: true
---

> 본 포스트는 "파이썬과 케라스로 배우는 강화학습" 도서의 네번째 리뷰 포스트입니다.
> http://www.yes24.com/Product/Goods/44136413

## 3장 강화학습 기초 2: 그리드월드와 다이내믹 프로그래밍

---

지금까지 열심히 강화학습 문제란 무엇인지, 문제를 어떻게 수학적으로 정의할 수 있는지, 그러한 문제의 최적의 방정식은 어떻게 구성되었는지를 살펴보았다. 이제 본격적으로 예제와 함께 문제를 풀어볼 시간이다 👉

`다이내믹 프로그래밍(Dynamic Programming)`은 작은 문제가 큰 문제 안에 중첩되어 있는 경우, 작은 문제의 답을 다른 작은 문제에서 이용함으로써 효율적으로 계산하는 방법이다.

다이내믹 프로그래밍을 이용하여,

- **벨만 기대 방정식** 을 푸는 것 --> `정책 이터레이션`
- **벨만 최적 방정식** 을 푸는 것 --> `가치 이터레이션`

이며, 이번 장에서는 정책 이터레이션과 가치 이터레이션을 그리드월드 예제를 통해 코드로 실습해본다.

> 다이내믹 프로그래밍은 이후에 강화학습의 근간이 되기 때문에 제대로 이해하는 것이 중요하다!!

### 순차적 행동 결정 문제

**강화학습** 은 순차적으로 행동을 결정해야 하는 문제를 푸는 방법중 하나 이다. 2장에서 **MDP** 를 정의하고 **벨만 방정식** 을 세우는 과정을 다뤘다. 순차적 행동 결정 문제를 푸는 방법을 정리하면 아래와 같다.

1. 순차적 행동문제 --> MDP로 전환
2. 가치함수를 벨만 방정식으로 반복적으로 계산
3. 최적 가치함수와 최적 정책 도출

강화학습 또한 순차적 행동 결정 문제를 푸는 방법이기 때문에, 벨만 방정식을 이해해야 강화학습을 이해할 수 있다. 그렇다면 **벨만 방정식을 푼다는 것** 은 어떤 의미일까? 보통 수학에서 **"방정식을 푼다"** 라고 하면 **식을 만족하는 변수의 값을 찾는 것** 을 말한다. 벨만 방정식을 통해 에이전트가 하고 싶은 것은 아래의 식을 만족하는 $v^*$을 찾는 것이다. 이 값을 찾는다면 벨만 방정식은 풀린 것이며 에이전트는 `최적 가치함수`를 알아낸 것이다.

$$v^*(s) = max_{a}E[R_{t+1} + \gamma v^*(S_{t+1}) | S_t=s, A_t=a]$$

### 다이내믹 프로그래밍

다이내믹 프로그래밍의 기본적인 아이디어는 큰 문제 안에 작은 문제들이 중첩된 경우, 전체 큰 문제를 작은 문제로 쪼개서 풀겠다는 것이다. 이때 각각의 작은 문제들이 별개가 아니기 때문에 작은 문제들의 해답을 서로서로 이용할 수 있다. 이 특성을 이용하면 **결과적으로 계산량을 줄일 수 있다.** (책에는 꽤 길게 서술되어 있는데 조금 생략했다)

문제의 목표는 각 상태의 참 가치함수를 구하는 것이다. 즉, $v_\pi (s_1), v_\pi (s_2), v_\pi (s_3) ...$의 참값을 구하는 것이다. 이 큰 문제를 작은 문제로 나누어서 구하게 되면, 아래와 같이 풀 수 있다.

$$v_0(s) \to v_1(s) \to ... \to v_k(s) \to ... \to v_\pi (s)$$

한번의 화살표는 한 번의 계산으로서 $iteration = k$ 에서 $iteration=k+1$이 되는 과정이다. 이 계산은 모든 상태에 대해 진행하며 한번 계산이 끝나면 모든 상태의 가치함수를 업데이트한다. 다음 계산은 업데이트된 가치함수를 이용해 다시 똑같은 과정을 반복하는 것이다. 이런 식으로 계산하면 이전의 정보를 이용해 효율적으로 업데이트할 수 있게 된다.

### 격자로 이뤄진 간단한 예제: 그리드월드

{% img /image/post_image/20210123/gridworld.png 'width:300px; height:300px' '' '그리드월드 예제' %}

예시로 우리가 풀어볼 문제는 다음과 같다. 빨간색 네모는 에이전트를 의미하고, 에이전트는 파란색 동그라미로 가야한다. 이때 (-1)의 보상을 주는 연두색 세모가 막고 있어, 세모를 피해서 네모에 도착해 (+1)의 보상을 받는 것이다. 파란색에 도착하는 **최적 정책** 을 찾는 것이 우리의 목표다.

### 강화학습 알고리즘의 흐름

MDP부터 강화학습의 기본적인 알고리즘까지 전반적인 흐름을 도식으로 보면 다음과 같다.

{% img /image/post_image/20210123/RLflow.png 'width:350px; height:400px' '' '그리드월드 예제' %}

자, 순차적 행동 결정 문제는 MDP를 통해 정의되었고, MDP로 정의되는 목표는 **에이전트가 받을 보상의 합을 최대** 로 하는 것 이며, 이는 벨만 방정식을 품으로써 달성할 수 있다. 또한 벨만 방정식은 다이내믹 프로그래밍을 통해 풀 수 있으며, 다이내믹 프로그래밍에는 `정책 이터레이션(policy iteration)`과 `가치 이터레이션(value iteration)`이 있다. 이 두 방법은 후에 `살사(SARSA)`로 발전하며, 살사는 `큐러닝(Q-Learning)`으로 이어진다.

이제 실제 코드를 통해 에이전트를 학습시켜보자!
